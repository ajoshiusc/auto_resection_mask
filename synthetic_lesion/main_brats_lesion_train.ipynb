{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.handlers.utils import from_engine\n",
    "from torch.nn import MSELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import UNet\n",
    "from make_data_utils import MakeLesionMaskedDatad\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    Resized,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"MONAI_DATA_DIRECTORY\"] = \"/deneb_disk/monai_data\"\n",
    "\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a new transform to convert brain tumor labels\n",
    "\n",
    "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 1 is the peritumoral edema\n",
    "    label 2 is the GD-enhancing tumor\n",
    "    label 3 is the necrotic and non-enhancing tumor core\n",
    "    The possible classes are TC (Tumor core), WT (Whole tumor)\n",
    "    and ET (Enhancing tumor).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 2 and label 3 to construct TC\n",
    "            result.append(torch.logical_or(d[key] == 2, d[key] == 3))\n",
    "            # merge labels 1, 2 and 3 to construct WT\n",
    "            result.append(\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            # label 2 is ET\n",
    "            result.append(d[key] == 2)\n",
    "            d[key] = torch.stack(result, axis=0).float()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        MakeLesionMaskedDatad(keys=[\"image\", \"label\"]),\n",
    "        # EnsureChannelFirstd(keys=[\"image\",\"label\"]),\n",
    "        #Spacingd(\n",
    "        #    keys=[\"image\", \"label\"],\n",
    "        #    pixdim=(1.0, 1.0, 1.0),\n",
    "        #    mode=(\"bilinear\", \"bilinear\"),\n",
    "        #),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=(64, 64, 64), mode=[\"bilinear\", \"bilinear\"]),\n",
    "        #RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        NormalizeIntensityd(keys=[\"image\",\"label\"], nonzero=True, channel_wise=True),\n",
    "\n",
    "        #RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "        #RandShiftIntensityd(keys=[\"image\", \"label\"], offsets=0.1, prob=1.0),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        #Resized(keys=[\"image\", \"label\"], spatial_size=(64, 64, 64), mode=[\"bilinear\", \"bilinear\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        MakeLesionMaskedDatad(keys=[\"image\", \"label\"]),\n",
    "        # EnsureChannelFirstd(keys=[\"image\",\"label\"]),\n",
    "        #Spacingd(\n",
    "        #    keys=[\"image\", \"label\"],\n",
    "        #    pixdim=(1.0, 1.0, 1.0),\n",
    "        #    mode=(\"bilinear\", \"nearest\"),\n",
    "        #),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=(64, 64, 64), mode=[\"bilinear\", \"bilinear\"]),\n",
    "        NormalizeIntensityd(keys=[\"image\", \"label\"], nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly load data with DecathlonDataset\n",
    "\n",
    "Here we use `DecathlonDataset` to automatically download and extract the dataset.\n",
    "It inherits MONAI `CacheDataset`, if you want to use less memory, you can set `cache_num=N` to cache N items for training and use the default args to cache all the items for validation, it depends on your memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we don't cache any data in case out of memory issue\n",
    "train_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=train_transform,\n",
    "    section=\"training\",\n",
    "    download=True,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from DecathlonDataset to visualize and check the 4 channels\n",
    "val_data_example = val_ds[0]\n",
    "SL = int(val_data_example[\"image\"].shape[3]/2)\n",
    "print(f\"image shape: {val_data_example['image'].shape}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f\"image\")\n",
    "plt.imshow(val_data_example[\"image\"][0, :, :, SL].detach().cpu(), cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f\"wolesion\")\n",
    "plt.imshow(val_data_example[\"label\"][0, :, :, SL].detach().cpu(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    channels=[16, 32, 16],\n",
    "    strides=[2, 2, 2, 2, 2],\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    ").to(device)\n",
    "loss_function = MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(64,64,64),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/auto_resection_mask/synthetic_lesion/main_brats_lesion_train.ipynb Cell 20\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/auto_resection_mask/synthetic_lesion/main_brats_lesion_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/auto_resection_mask/synthetic_lesion/main_brats_lesion_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/auto_resection_mask/synthetic_lesion/main_brats_lesion_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mfor\u001b[39;00m val_data \u001b[39min\u001b[39;00m val_ds:\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/auto_resection_mask/synthetic_lesion/main_brats_lesion_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m         val_inputs, val_labels \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/auto_resection_mask/synthetic_lesion/main_brats_lesion_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m             val_data[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/auto_resection_mask/synthetic_lesion/main_brats_lesion_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m             val_data[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/auto_resection_mask/synthetic_lesion/main_brats_lesion_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f616a6f7368692f50726f6a656374732f6175746f5f726573656374696f6e5f6d61736b2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/auto_resection_mask/synthetic_lesion/main_brats_lesion_train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m         \u001b[39m#print(val_inputs.shape)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/data/dataset.py:112\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[1;32m    110\u001b[0m     \u001b[39m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[39mreturn\u001b[39;00m Subset(dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, indices\u001b[39m=\u001b[39mindex)\n\u001b[0;32m--> 112\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform(index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/data/dataset.py:910\u001b[0m, in \u001b[0;36mCacheDataset._transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    906\u001b[0m     cache_index \u001b[39m=\u001b[39m index\n\u001b[1;32m    908\u001b[0m \u001b[39mif\u001b[39;00m cache_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m     \u001b[39m# no cache for this index, execute all the transforms directly\u001b[39;00m\n\u001b[0;32m--> 910\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_transform(index)\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    913\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcache buffer is not initialized, please call `set_data()` first.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/data/dataset.py:98\u001b[0m, in \u001b[0;36mDataset._transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mFetch single data item from `self.data`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m data_i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index]\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m apply_transform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform, data_i) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m data_i\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m map_items:\n\u001b[1;32m    140\u001b[0m         \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n\u001b[1;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[39m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39m# appears where the exception was raised.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m MONAIEnvVars\u001b[39m.\u001b[39mdebug():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m unpack_parameters:\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m transform(\u001b[39m*\u001b[39mdata, lazy\u001b[39m=\u001b[39mlazy) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, LazyTrait) \u001b[39melse\u001b[39;00m transform(\u001b[39m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m transform(data, lazy\u001b[39m=\u001b[39mlazy) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, LazyTrait) \u001b[39melse\u001b[39;00m transform(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/transforms/compose.py:335\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_, start\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, threading\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, lazy: \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m     _lazy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lazy \u001b[39mif\u001b[39;00m lazy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lazy\n\u001b[0;32m--> 335\u001b[0m     result \u001b[39m=\u001b[39m execute_compose(\n\u001b[1;32m    336\u001b[0m         input_,\n\u001b[1;32m    337\u001b[0m         transforms\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms,\n\u001b[1;32m    338\u001b[0m         start\u001b[39m=\u001b[39mstart,\n\u001b[1;32m    339\u001b[0m         end\u001b[39m=\u001b[39mend,\n\u001b[1;32m    340\u001b[0m         map_items\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_items,\n\u001b[1;32m    341\u001b[0m         unpack_items\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munpack_items,\n\u001b[1;32m    342\u001b[0m         lazy\u001b[39m=\u001b[39m_lazy,\n\u001b[1;32m    343\u001b[0m         overrides\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverrides,\n\u001b[1;32m    344\u001b[0m         threading\u001b[39m=\u001b[39mthreading,\n\u001b[1;32m    345\u001b[0m         log_stats\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_stats,\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    348\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[39m=\u001b[39m deepcopy(_transform) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[39melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[39m=\u001b[39m apply_transform(\n\u001b[1;32m    112\u001b[0m         _transform, data, map_items, unpack_items, lazy\u001b[39m=\u001b[39mlazy, overrides\u001b[39m=\u001b[39moverrides, log_stats\u001b[39m=\u001b[39mlog_stats\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m data \u001b[39m=\u001b[39m apply_pending_transforms(data, \u001b[39mNone\u001b[39;00m, overrides, logger_name\u001b[39m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m map_items:\n\u001b[1;32m    140\u001b[0m         \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n\u001b[1;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[39m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39m# appears where the exception was raised.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m MONAIEnvVars\u001b[39m.\u001b[39mdebug():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m unpack_parameters:\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m transform(\u001b[39m*\u001b[39mdata, lazy\u001b[39m=\u001b[39mlazy) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, LazyTrait) \u001b[39melse\u001b[39;00m transform(\u001b[39m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m transform(data, lazy\u001b[39m=\u001b[39mlazy) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, LazyTrait) \u001b[39melse\u001b[39;00m transform(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/transforms/io/dictionary.py:162\u001b[0m, in \u001b[0;36mLoadImaged.__call__\u001b[0;34m(self, data, reader)\u001b[0m\n\u001b[1;32m    160\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(data)\n\u001b[1;32m    161\u001b[0m \u001b[39mfor\u001b[39;00m key, meta_key, meta_key_postfix \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_iterator(d, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_keys, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_key_postfix):\n\u001b[0;32m--> 162\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader(d[key], reader)\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loader\u001b[39m.\u001b[39mimage_only:\n\u001b[1;32m    164\u001b[0m         d[key] \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/transforms/io/array.py:282\u001b[0m, in \u001b[0;36mLoadImage.__call__\u001b[0;34m(self, filename, reader)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    275\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m cannot find a suitable reader for file: \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m    Please install the reader libraries, see also the installation instructions:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m   The current registered: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreaders\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    281\u001b[0m img_array: NdarrayOrTensor\n\u001b[0;32m--> 282\u001b[0m img_array, meta_data \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39mget_data(img)\n\u001b[1;32m    283\u001b[0m img_array \u001b[39m=\u001b[39m convert_to_dst_type(img_array, dst\u001b[39m=\u001b[39mimg_array, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(meta_data, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/data/image_reader.py:938\u001b[0m, in \u001b[0;36mNibabelReader.get_data\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    936\u001b[0m header[MetaKeys\u001b[39m.\u001b[39mSPATIAL_SHAPE] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_spatial_shape(i)\n\u001b[1;32m    937\u001b[0m header[MetaKeys\u001b[39m.\u001b[39mSPACE] \u001b[39m=\u001b[39m SpaceKeys\u001b[39m.\u001b[39mRAS\n\u001b[0;32m--> 938\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_array_data(i)\n\u001b[1;32m    939\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqueeze_non_spatial_dims:\n\u001b[1;32m    940\u001b[0m     \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape), \u001b[39mlen\u001b[39m(header[MetaKeys\u001b[39m.\u001b[39mSPATIAL_SHAPE]), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/monai/data/image_reader.py:1012\u001b[0m, in \u001b[0;36mNibabelReader._get_array_data\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_array_data\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m   1005\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[39m    Get the raw array data of the image, converted to Numpy array.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \n\u001b[1;32m   1011\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masanyarray(img\u001b[39m.\u001b[39mdataobj, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nibabel/arrayproxy.py:439\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    419\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[1;32m    421\u001b[0m \u001b[39m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_scaled(dtype\u001b[39m=\u001b[39mdtype, slicer\u001b[39m=\u001b[39m())\n\u001b[1;32m    440\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m         arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nibabel/arrayproxy.py:406\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    404\u001b[0m     scl_inter \u001b[39m=\u001b[39m scl_inter\u001b[39m.\u001b[39mastype(use_dtype)\n\u001b[1;32m    405\u001b[0m \u001b[39m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m scaled \u001b[39m=\u001b[39m apply_read_scaling(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_unscaled(slicer\u001b[39m=\u001b[39mslicer), scl_slope, scl_inter)\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     scaled \u001b[39m=\u001b[39m scaled\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mpromote_types(scaled\u001b[39m.\u001b[39mdtype, dtype), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nibabel/arrayproxy.py:376\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m canonical_slicers(slicer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape, \u001b[39mFalse\u001b[39;00m) \u001b[39m==\u001b[39m canonical_slicers(\n\u001b[1;32m    373\u001b[0m     (), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    374\u001b[0m ):\n\u001b[1;32m    375\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_fileobj() \u001b[39mas\u001b[39;00m fileobj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 376\u001b[0m         \u001b[39mreturn\u001b[39;00m array_from_file(\n\u001b[1;32m    377\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape,\n\u001b[1;32m    378\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype,\n\u001b[1;32m    379\u001b[0m             fileobj,\n\u001b[1;32m    380\u001b[0m             offset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset,\n\u001b[1;32m    381\u001b[0m             order\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morder,\n\u001b[1;32m    382\u001b[0m             mmap\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mmap,\n\u001b[1;32m    383\u001b[0m         )\n\u001b[1;32m    384\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_fileobj() \u001b[39mas\u001b[39;00m fileobj:\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m fileslice(\n\u001b[1;32m    386\u001b[0m         fileobj,\n\u001b[1;32m    387\u001b[0m         slicer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m         lock\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock,\n\u001b[1;32m    393\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nibabel/volumeutils.py:465\u001b[0m, in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(infile, \u001b[39m'\u001b[39m\u001b[39mreadinto\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    464\u001b[0m     data_bytes \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(n_bytes)\n\u001b[0;32m--> 465\u001b[0m     n_read \u001b[39m=\u001b[39m infile\u001b[39m.\u001b[39mreadinto(data_bytes)\n\u001b[1;32m    466\u001b[0m     needs_copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39merrno\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(errno\u001b[39m.\u001b[39mEBADF, \u001b[39m\"\u001b[39m\u001b[39mread() on write-only GzipFile object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer\u001b[39m.\u001b[39mread(size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/gzip.py:521\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[39mif\u001b[39;00m buf \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCompressed file ended before the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mend-of-stream marker was reached\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_read_data( uncompress )\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(uncompress)\n\u001b[1;32m    523\u001b[0m \u001b[39mreturn\u001b[39;00m uncompress\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/gzip.py:526\u001b[0m, in \u001b[0;36m_GzipReader._add_read_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_add_read_data\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m--> 526\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_crc \u001b[39m=\u001b[39m zlib\u001b[39m.\u001b[39mcrc32(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_crc)\n\u001b[1;32m    527\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream_size \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_metric = 1e6\n",
    "best_metric_epoch = 1e6\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_ds:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        print(f'input shape: {inputs.shape}')\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs[None, ...])\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "\n",
    "        #model.eval()\n",
    "        #with torch.no_grad():\n",
    "        #    for val_data in val_ds:\n",
    "        #        val_inputs, val_labels = (\n",
    "        #            val_data[\"image\"].to(device),\n",
    "        #            val_data[\"label\"].to(device),\n",
    "        #        )\n",
    "        #        val_outputs = inference(val_inputs[None,...])\n",
    " \n",
    "\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_ds:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                #print(val_inputs.shape)\n",
    "                val_outputs = inference(val_inputs[None,...])\n",
    "                #val_outputs = [post_trans(i)\n",
    "                #               for i in decollate_batch(val_outputs)]\n",
    "                loss = loss_function(val_outputs, val_labels)\n",
    "\n",
    "            metric = loss.item()\n",
    "            metric_values.append(metric)\n",
    "            #metric_batch = dice_metric_batch.aggregate()\n",
    "            #metric_tc = metric_batch[0].item()\n",
    "            #metric_values_tc.append(metric_tc)\n",
    "            # metric_wt = metric_batch[1].item()\n",
    "            # metric_values_wt.append(metric_wt)\n",
    "            # metric_et = metric_batch[2].item()\n",
    "            # metric_values_et.append(metric_et)\n",
    "            #metric.reset()\n",
    "            #dice_metric_batch.reset()\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(\n",
    "                root_dir, \"latest_model.pth\"))\n",
    "\n",
    "            if metric < best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(\n",
    "                    time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mse: {metric:.4f}\"\n",
    "                f\" tc: {metric:.4f}\"\n",
    "                f\"\\nbest mse: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(\n",
    "        f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input = val_ds[6][\"image\"].unsqueeze(0).to(device)\n",
    "    roi_size = (128, 128, 64)\n",
    "    val_output = inference(val_input, roi_size=roi_size)\n",
    "    val_output = val_output[0]\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on original image spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_org_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_org_transforms,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    num_workers=4,\n",
    "    cache_num=0,\n",
    ")\n",
    "val_org_loader = DataLoader(\n",
    "    val_org_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "post_transforms = Compose(\n",
    "    [\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=val_org_transforms,\n",
    "            orig_keys=\"image\",\n",
    "            meta_keys=\"pred_meta_dict\",\n",
    "            orig_meta_keys=\"image_meta_dict\",\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "            device=\"cpu\",\n",
    "        ),\n",
    "        Activationsd(keys=\"pred\", sigmoid=True),\n",
    "        AsDiscreted(keys=\"pred\", threshold=0.5),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_data in val_org_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        val_data[\"pred\"] = inference(val_inputs)\n",
    "        val_data = [post_transforms(i) for i in decollate_batch(val_data)]\n",
    "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "    metric_org = dice_metric.aggregate().item()\n",
    "    metric_batch_org = dice_metric_batch.aggregate()\n",
    "\n",
    "    dice_metric.reset()\n",
    "    dice_metric_batch.reset()\n",
    "\n",
    "metric_tc, metric_wt, metric_et = (\n",
    "    metric_batch_org[0].item(),\n",
    "    metric_batch_org[1].item(),\n",
    "    metric_batch_org[2].item(),\n",
    ")\n",
    "\n",
    "print(\"Metric on original image spacing: \", metric_org)\n",
    "print(f\"metric_tc: {metric_tc:.4f}\")\n",
    "print(f\"metric_wt: {metric_wt:.4f}\")\n",
    "print(f\"metric_et: {metric_et:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
