{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajoshi/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_hip.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.0\n",
      "Numpy version: 1.24.3\n",
      "Pytorch version: 2.0.1+cu118\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
      "MONAI __file__: /home/<username>/anaconda3/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.1.0\n",
      "scikit-image version: 0.20.0\n",
      "scipy version: 1.11.3\n",
      "Pillow version: 10.0.1\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.15.2+rocm5.4.2\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.1.1\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: 4.32.1\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.handlers.utils import from_engine\n",
    "from torch.nn import MSELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import UNet\n",
    "from make_data_utils import MakeLesionMaskedDatad\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    Resized,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/deneb_disk/monai_data\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"MONAI_DATA_DIRECTORY\"] = \"/deneb_disk/monai_data\"\n",
    "\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a new transform to convert brain tumor labels\n",
    "\n",
    "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 1 is the peritumoral edema\n",
    "    label 2 is the GD-enhancing tumor\n",
    "    label 3 is the necrotic and non-enhancing tumor core\n",
    "    The possible classes are TC (Tumor core), WT (Whole tumor)\n",
    "    and ET (Enhancing tumor).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 2 and label 3 to construct TC\n",
    "            result.append(torch.logical_or(d[key] == 2, d[key] == 3))\n",
    "            # merge labels 1, 2 and 3 to construct WT\n",
    "            result.append(\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            # label 2 is ET\n",
    "            result.append(d[key] == 2)\n",
    "            d[key] = torch.stack(result, axis=0).float()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        MakeLesionMaskedDatad(keys=[\"image\", \"label\"]),\n",
    "        # EnsureChannelFirstd(keys=[\"image\",\"label\"]),\n",
    "        #Spacingd(\n",
    "        #    keys=[\"image\", \"label\"],\n",
    "        #    pixdim=(1.0, 1.0, 1.0),\n",
    "        #    mode=(\"bilinear\", \"bilinear\"),\n",
    "        #),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=(64, 64, 64), mode=[\"bilinear\", \"bilinear\"]),\n",
    "        #RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        NormalizeIntensityd(keys=[\"image\",\"label\"], nonzero=True, channel_wise=True),\n",
    "\n",
    "        #RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "        #RandShiftIntensityd(keys=[\"image\", \"label\"], offsets=0.1, prob=1.0),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        #Resized(keys=[\"image\", \"label\"], spatial_size=(64, 64, 64), mode=[\"bilinear\", \"bilinear\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        MakeLesionMaskedDatad(keys=[\"image\", \"label\"]),\n",
    "        # EnsureChannelFirstd(keys=[\"image\",\"label\"]),\n",
    "        #Spacingd(\n",
    "        #    keys=[\"image\", \"label\"],\n",
    "        #    pixdim=(1.0, 1.0, 1.0),\n",
    "        #    mode=(\"bilinear\", \"nearest\"),\n",
    "        #),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=(64, 64, 64), mode=[\"bilinear\", \"bilinear\"]),\n",
    "        NormalizeIntensityd(keys=[\"image\", \"label\"], nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly load data with DecathlonDataset\n",
    "\n",
    "Here we use `DecathlonDataset` to automatically download and extract the dataset.\n",
    "It inherits MONAI `CacheDataset`, if you want to use less memory, you can set `cache_num=N` to cache N items for training and use the default args to cache all the items for validation, it depends on your memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-30 13:39:08,830 - INFO - Verified 'Task01_BrainTumour.tar', md5: 240a19d752f0d9e9101544901065d872.\n",
      "2023-10-30 13:39:08,835 - INFO - File exists: /deneb_disk/monai_data/Task01_BrainTumour.tar, skipped downloading.\n",
      "2023-10-30 13:39:08,837 - INFO - Non-empty folder exists in /deneb_disk/monai_data/Task01_BrainTumour, skipped extracting.\n"
     ]
    }
   ],
   "source": [
    "# here we don't cache any data in case out of memory issue\n",
    "train_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=train_transform,\n",
    "    section=\"training\",\n",
    "    download=True,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([1, 64, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pick one image from DecathlonDataset to visualize and check the 4 channels\n",
    "val_data_example = train_ds[0]\n",
    "SL = int(val_data_example[\"image\"].shape[3]/2)\n",
    "print(f\"image shape: {val_data_example['image'].shape}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f\"image\")\n",
    "plt.imshow(val_data_example[\"image\"][0, :, :, SL].detach().cpu(), cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f\"wolesion\")\n",
    "plt.imshow(val_data_example[\"label\"][0, :, :, SL].detach().cpu(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(SL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajoshi/anaconda3/lib/python3.11/site-packages/monai/networks/nets/unet.py:133: UserWarning: `len(strides) > len(channels) - 1`, the last 3 values of strides will not be used.\n",
      "  warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    channels=[16, 32, 16],\n",
    "    strides=[2, 2, 2, 2, 2],\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    ").to(device)\n",
    "loss_function = MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(64, 64, 64),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "1/388, train_loss: 0.8304, step time: 0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajoshi/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:1295: UserWarning: Using a target size (torch.Size([1, 64, 64, 64])) that is different to the input size (torch.Size([1, 1, 64, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "2/388, train_loss: 0.8137, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "3/388, train_loss: 0.7864, step time: 0.0131\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "4/388, train_loss: 0.7809, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "5/388, train_loss: 0.7459, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "6/388, train_loss: 0.7075, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "7/388, train_loss: 0.7122, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "8/388, train_loss: 0.7252, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "9/388, train_loss: 0.6810, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "10/388, train_loss: 0.6443, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "11/388, train_loss: 0.6473, step time: 0.0097\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "12/388, train_loss: 0.6474, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "13/388, train_loss: 0.6364, step time: 0.0108\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "14/388, train_loss: 0.6228, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "15/388, train_loss: 0.6039, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "16/388, train_loss: 0.5758, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "17/388, train_loss: 0.6137, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "18/388, train_loss: 0.5940, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "19/388, train_loss: 0.6226, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "20/388, train_loss: 0.5283, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "21/388, train_loss: 0.5362, step time: 0.0096\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "22/388, train_loss: 0.5170, step time: 0.0095\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "23/388, train_loss: 0.5442, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "24/388, train_loss: 0.5313, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "25/388, train_loss: 0.5182, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "26/388, train_loss: 0.5135, step time: 0.0097\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "27/388, train_loss: 0.5085, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "28/388, train_loss: 0.5172, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "29/388, train_loss: 0.5143, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "30/388, train_loss: 0.5341, step time: 0.0113\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "31/388, train_loss: 0.4706, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "32/388, train_loss: 0.4563, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "33/388, train_loss: 0.5077, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "34/388, train_loss: 0.4538, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "35/388, train_loss: 0.4439, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "36/388, train_loss: 0.4593, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "37/388, train_loss: 0.4984, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "38/388, train_loss: 0.4992, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "39/388, train_loss: 0.4537, step time: 0.0093\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "40/388, train_loss: 0.4580, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "41/388, train_loss: 0.4354, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "42/388, train_loss: 0.4439, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "43/388, train_loss: 0.4448, step time: 0.0085\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "44/388, train_loss: 0.3919, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "45/388, train_loss: 0.4419, step time: 0.0085\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "46/388, train_loss: 0.4574, step time: 0.0228\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "47/388, train_loss: 0.4097, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "48/388, train_loss: 0.4163, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "49/388, train_loss: 0.4151, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "50/388, train_loss: 0.3887, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "51/388, train_loss: 0.4403, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "52/388, train_loss: 0.4413, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "53/388, train_loss: 0.4258, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "54/388, train_loss: 0.4159, step time: 0.0093\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "55/388, train_loss: 0.3927, step time: 0.0108\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "56/388, train_loss: 0.3680, step time: 0.0095\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "57/388, train_loss: 0.3865, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "58/388, train_loss: 0.3761, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "59/388, train_loss: 0.3677, step time: 0.0167\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "60/388, train_loss: 0.3885, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "61/388, train_loss: 0.3707, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "62/388, train_loss: 0.3344, step time: 0.0110\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "63/388, train_loss: 0.3496, step time: 0.0135\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "64/388, train_loss: 0.3749, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "65/388, train_loss: 0.3730, step time: 0.0125\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "66/388, train_loss: 0.3902, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "67/388, train_loss: 0.3870, step time: 0.0118\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "68/388, train_loss: 0.3620, step time: 0.0085\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "69/388, train_loss: 0.3580, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "70/388, train_loss: 0.3620, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "71/388, train_loss: 0.3611, step time: 0.0131\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "72/388, train_loss: 0.3445, step time: 0.0119\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "73/388, train_loss: 0.3487, step time: 0.0101\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "74/388, train_loss: 0.3507, step time: 0.0103\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "75/388, train_loss: 0.3775, step time: 0.0094\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "76/388, train_loss: 0.3737, step time: 0.0105\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "77/388, train_loss: 0.3600, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "78/388, train_loss: 0.3548, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "79/388, train_loss: 0.3363, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "80/388, train_loss: 0.3615, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "81/388, train_loss: 0.3491, step time: 0.0096\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "82/388, train_loss: 0.3135, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "83/388, train_loss: 0.3050, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "84/388, train_loss: 0.3047, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "85/388, train_loss: 0.3384, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "86/388, train_loss: 0.3176, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "87/388, train_loss: 0.3222, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "88/388, train_loss: 0.3331, step time: 0.0114\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "89/388, train_loss: 0.3359, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "90/388, train_loss: 0.3435, step time: 0.0121\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "91/388, train_loss: 0.3166, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "92/388, train_loss: 0.3157, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "93/388, train_loss: 0.3297, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "94/388, train_loss: 0.3177, step time: 0.0101\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "95/388, train_loss: 0.3157, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "96/388, train_loss: 0.3077, step time: 0.0097\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "97/388, train_loss: 0.3152, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "98/388, train_loss: 0.3008, step time: 0.0107\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "99/388, train_loss: 0.3347, step time: 0.0111\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "100/388, train_loss: 0.3011, step time: 0.0101\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "101/388, train_loss: 0.2939, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "102/388, train_loss: 0.3030, step time: 0.0093\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "103/388, train_loss: 0.2476, step time: 0.0108\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "104/388, train_loss: 0.3011, step time: 0.0109\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "105/388, train_loss: 0.3043, step time: 0.0107\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "106/388, train_loss: 0.2869, step time: 0.0096\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "107/388, train_loss: 0.2954, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "108/388, train_loss: 0.2922, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "109/388, train_loss: 0.3089, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "110/388, train_loss: 0.3008, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "111/388, train_loss: 0.3135, step time: 0.0107\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "112/388, train_loss: 0.2913, step time: 0.0108\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "113/388, train_loss: 0.2859, step time: 0.0107\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "114/388, train_loss: 0.2912, step time: 0.0116\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "115/388, train_loss: 0.3171, step time: 0.0111\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "116/388, train_loss: 0.2824, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "117/388, train_loss: 0.2898, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "118/388, train_loss: 0.2943, step time: 0.0096\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "119/388, train_loss: 0.2733, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "120/388, train_loss: 0.2764, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "121/388, train_loss: 0.2744, step time: 0.0120\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "122/388, train_loss: 0.2872, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "123/388, train_loss: 0.2687, step time: 0.0095\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "124/388, train_loss: 0.2746, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "125/388, train_loss: 0.2677, step time: 0.0107\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "126/388, train_loss: 0.2671, step time: 0.0107\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "127/388, train_loss: 0.2950, step time: 0.0107\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "128/388, train_loss: 0.2490, step time: 0.0121\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "129/388, train_loss: 0.2699, step time: 0.0121\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "130/388, train_loss: 0.2685, step time: 0.0096\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "131/388, train_loss: 0.2591, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "132/388, train_loss: 0.2557, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "133/388, train_loss: 0.2410, step time: 0.0101\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "134/388, train_loss: 0.2995, step time: 0.0116\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "135/388, train_loss: 0.2370, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "136/388, train_loss: 0.2755, step time: 0.0114\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "137/388, train_loss: 0.2903, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "138/388, train_loss: 0.2775, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "139/388, train_loss: 0.2522, step time: 0.0097\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "140/388, train_loss: 0.2603, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "141/388, train_loss: 0.2870, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "142/388, train_loss: 0.2498, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "143/388, train_loss: 0.2670, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "144/388, train_loss: 0.2413, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "145/388, train_loss: 0.2288, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "146/388, train_loss: 0.2151, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "147/388, train_loss: 0.2356, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "148/388, train_loss: 0.2782, step time: 0.0097\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "149/388, train_loss: 0.2444, step time: 0.0101\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "150/388, train_loss: 0.2235, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "151/388, train_loss: 0.2358, step time: 0.0097\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "152/388, train_loss: 0.2299, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "153/388, train_loss: 0.2274, step time: 0.0097\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "154/388, train_loss: 0.2415, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "155/388, train_loss: 0.2875, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "156/388, train_loss: 0.2538, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "157/388, train_loss: 0.2381, step time: 0.0095\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "158/388, train_loss: 0.2363, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "159/388, train_loss: 0.2239, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "160/388, train_loss: 0.2425, step time: 0.0085\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "161/388, train_loss: 0.2347, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "162/388, train_loss: 0.2516, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "163/388, train_loss: 0.2783, step time: 0.0085\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "164/388, train_loss: 0.2485, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "165/388, train_loss: 0.2643, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "166/388, train_loss: 0.2301, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "167/388, train_loss: 0.2479, step time: 0.0083\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "168/388, train_loss: 0.2415, step time: 0.0096\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "169/388, train_loss: 0.2280, step time: 0.0119\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "170/388, train_loss: 0.2303, step time: 0.0095\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "171/388, train_loss: 0.2243, step time: 0.0109\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "172/388, train_loss: 0.2275, step time: 0.0103\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "173/388, train_loss: 0.2336, step time: 0.0093\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "174/388, train_loss: 0.2500, step time: 0.0093\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "175/388, train_loss: 0.2430, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "176/388, train_loss: 0.2399, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "177/388, train_loss: 0.2414, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "178/388, train_loss: 0.2267, step time: 0.0105\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "179/388, train_loss: 0.1836, step time: 0.0108\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "180/388, train_loss: 0.2031, step time: 0.0105\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "181/388, train_loss: 0.2264, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "182/388, train_loss: 0.2165, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "183/388, train_loss: 0.2276, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "184/388, train_loss: 0.2208, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "185/388, train_loss: 0.2150, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "186/388, train_loss: 0.2285, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "187/388, train_loss: 0.1935, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "188/388, train_loss: 0.1996, step time: 0.0095\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "189/388, train_loss: 0.2159, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "190/388, train_loss: 0.2123, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "191/388, train_loss: 0.2259, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "192/388, train_loss: 0.1932, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "193/388, train_loss: 0.2302, step time: 0.0101\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "194/388, train_loss: 0.2113, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "195/388, train_loss: 0.2068, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "196/388, train_loss: 0.2320, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "197/388, train_loss: 0.2090, step time: 0.0116\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "198/388, train_loss: 0.2050, step time: 0.0110\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "199/388, train_loss: 0.2348, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "200/388, train_loss: 0.1710, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "201/388, train_loss: 0.2315, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "202/388, train_loss: 0.2018, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "203/388, train_loss: 0.1884, step time: 0.0109\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "204/388, train_loss: 0.2313, step time: 0.0124\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "205/388, train_loss: 0.1931, step time: 0.0117\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "206/388, train_loss: 0.2114, step time: 0.0112\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "207/388, train_loss: 0.1873, step time: 0.0116\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "208/388, train_loss: 0.1980, step time: 0.0113\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "209/388, train_loss: 0.2159, step time: 0.0107\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "210/388, train_loss: 0.1730, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "211/388, train_loss: 0.1997, step time: 0.0106\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "212/388, train_loss: 0.1933, step time: 0.0121\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "213/388, train_loss: 0.2114, step time: 0.0136\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "214/388, train_loss: 0.2022, step time: 0.0114\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "215/388, train_loss: 0.1897, step time: 0.0116\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "216/388, train_loss: 0.2118, step time: 0.0116\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "217/388, train_loss: 0.2171, step time: 0.0134\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "218/388, train_loss: 0.1944, step time: 0.0110\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "219/388, train_loss: 0.2180, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "220/388, train_loss: 0.2027, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "221/388, train_loss: 0.1691, step time: 0.0115\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "222/388, train_loss: 0.1678, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "223/388, train_loss: 0.1914, step time: 0.0115\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "224/388, train_loss: 0.1878, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "225/388, train_loss: 0.2452, step time: 0.0095\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "226/388, train_loss: 0.1853, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "227/388, train_loss: 0.1963, step time: 0.0103\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "228/388, train_loss: 0.1790, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "229/388, train_loss: 0.2099, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "230/388, train_loss: 0.1898, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "231/388, train_loss: 0.1882, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "232/388, train_loss: 0.1958, step time: 0.0113\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "233/388, train_loss: 0.1827, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "234/388, train_loss: 0.2101, step time: 0.0108\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "235/388, train_loss: 0.1971, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "236/388, train_loss: 0.1679, step time: 0.0092\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "237/388, train_loss: 0.1813, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "238/388, train_loss: 0.1827, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "239/388, train_loss: 0.1875, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "240/388, train_loss: 0.1935, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "241/388, train_loss: 0.1664, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "242/388, train_loss: 0.1723, step time: 0.0105\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "243/388, train_loss: 0.1893, step time: 0.0101\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "244/388, train_loss: 0.1833, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "245/388, train_loss: 0.1528, step time: 0.0111\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "246/388, train_loss: 0.1968, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "247/388, train_loss: 0.1625, step time: 0.0110\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "248/388, train_loss: 0.1616, step time: 0.0098\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "249/388, train_loss: 0.1702, step time: 0.0094\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "250/388, train_loss: 0.1864, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "251/388, train_loss: 0.2028, step time: 0.0084\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "252/388, train_loss: 0.1935, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "253/388, train_loss: 0.1721, step time: 0.0091\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "254/388, train_loss: 0.1863, step time: 0.0116\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "255/388, train_loss: 0.1688, step time: 0.0097\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "256/388, train_loss: 0.2102, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "257/388, train_loss: 0.1885, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "258/388, train_loss: 0.1806, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "259/388, train_loss: 0.1532, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "260/388, train_loss: 0.1920, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "261/388, train_loss: 0.1679, step time: 0.0090\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "262/388, train_loss: 0.1602, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "263/388, train_loss: 0.1948, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "264/388, train_loss: 0.1591, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "265/388, train_loss: 0.1774, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "266/388, train_loss: 0.1778, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "267/388, train_loss: 0.1748, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "268/388, train_loss: 0.1576, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "269/388, train_loss: 0.2038, step time: 0.0089\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "270/388, train_loss: 0.1559, step time: 0.0087\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "271/388, train_loss: 0.2073, step time: 0.0100\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "272/388, train_loss: 0.1721, step time: 0.0119\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "273/388, train_loss: 0.1743, step time: 0.0113\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "274/388, train_loss: 0.1850, step time: 0.0105\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "275/388, train_loss: 0.1865, step time: 0.0099\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "276/388, train_loss: 0.1786, step time: 0.0115\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "277/388, train_loss: 0.1535, step time: 0.0110\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "278/388, train_loss: 0.1389, step time: 0.0102\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "279/388, train_loss: 0.1718, step time: 0.0088\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "280/388, train_loss: 0.1796, step time: 0.0085\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "281/388, train_loss: 0.1548, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "282/388, train_loss: 0.1636, step time: 0.0086\n",
      "input shape: torch.Size([1, 64, 64, 64])\n",
      "283/388, train_loss: 0.1119, step time: 0.0104\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_ds:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        print(f'input shape: {inputs.shape}')\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs[None, ...])\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_ds:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i)\n",
    "                               for i in decollate_batch(val_outputs)]\n",
    "                loss = loss_function(val_outputs, val_labels)\n",
    "\n",
    "            metric = loss.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            #metric_batch = dice_metric_batch.aggregate()\n",
    "            #metric_tc = metric_batch[0].item()\n",
    "            #metric_values_tc.append(metric_tc)\n",
    "            # metric_wt = metric_batch[1].item()\n",
    "            # metric_values_wt.append(metric_wt)\n",
    "            # metric_et = metric_batch[2].item()\n",
    "            # metric_values_et.append(metric_et)\n",
    "            metric.reset()\n",
    "            #dice_metric_batch.reset()\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(\n",
    "                root_dir, \"latest_model.pth\"))\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(\n",
    "                    time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mse: {metric:.4f}\"\n",
    "                f\" tc: {metric:.4f}\"\n",
    "                f\"\\nbest mse: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(\n",
    "        f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input = val_ds[6][\"image\"].unsqueeze(0).to(device)\n",
    "    roi_size = (128, 128, 64)\n",
    "    val_output = inference(val_input, roi_size=roi_size)\n",
    "    val_output = val_output[0]\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on original image spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_org_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_org_transforms,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    num_workers=4,\n",
    "    cache_num=0,\n",
    ")\n",
    "val_org_loader = DataLoader(\n",
    "    val_org_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "post_transforms = Compose(\n",
    "    [\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=val_org_transforms,\n",
    "            orig_keys=\"image\",\n",
    "            meta_keys=\"pred_meta_dict\",\n",
    "            orig_meta_keys=\"image_meta_dict\",\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "            device=\"cpu\",\n",
    "        ),\n",
    "        Activationsd(keys=\"pred\", sigmoid=True),\n",
    "        AsDiscreted(keys=\"pred\", threshold=0.5),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_data in val_org_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        val_data[\"pred\"] = inference(val_inputs)\n",
    "        val_data = [post_transforms(i) for i in decollate_batch(val_data)]\n",
    "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "    metric_org = dice_metric.aggregate().item()\n",
    "    metric_batch_org = dice_metric_batch.aggregate()\n",
    "\n",
    "    dice_metric.reset()\n",
    "    dice_metric_batch.reset()\n",
    "\n",
    "metric_tc, metric_wt, metric_et = (\n",
    "    metric_batch_org[0].item(),\n",
    "    metric_batch_org[1].item(),\n",
    "    metric_batch_org[2].item(),\n",
    ")\n",
    "\n",
    "print(\"Metric on original image spacing: \", metric_org)\n",
    "print(f\"metric_tc: {metric_tc:.4f}\")\n",
    "print(f\"metric_wt: {metric_wt:.4f}\")\n",
    "print(f\"metric_et: {metric_et:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
